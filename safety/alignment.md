---
permalink: /safety/alignment/
---
# Value Alignment

The motivation to study AI alignment ([Yudkowsky, 2016](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)) is to build AI that behaves ethically so that smarter-than-human intelligence has a positive outcome ([Soares, 2017](https://intelligence.org/2017/04/12/ensuring/)). Ethical AI is still an open question ([Pavaloiu & Kose, 2017](https://arxiv.org/abs/1706.03021)).

[Lawrence (2017)](https://arxiv.org/abs/1705.07996) argues that we've already developed non-sentient, reactive machine intelligence that is aligned with our subconscious desires, but not our aspirations. Its effects, such as fake news, can confirm our biases and prejudice, undermine our spirit and soul.

* 2017 May 22, Neil D. Lawrence. [Living Together: Mind and Machine Intelligence](https://arxiv.org/abs/1705.07996). *arXiv:1705.07996*.
* 2017 May 16, Alice Pavaloiu and Utku Kose. [Ethical Artificial Intelligence - An Open Question](https://arxiv.org/abs/1706.03021). *arXiv:1706.03021*.
* 2017 April 12, Nate Soares. [Ensuring smarter-than-human intelligence has a positive outcome](https://intelligence.org/2017/04/12/ensuring/). *Machine Intelligence Research Institute*.
* 2016 December 28, Eliezer Yudkowsky. [AI Alignment: Why Itâ€™s Hard, and Where to Start](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/). *Machine Intelligence Research Institute*.
