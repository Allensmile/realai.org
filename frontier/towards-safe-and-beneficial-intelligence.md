---
permalink: /frontier/towards-safe-and-beneficial-intelligence.html
---
# Towards Safe and Beneficial Intelligence

*This article will be revised from time to time as our understanding of AI safety improves. When this page develops into a cohrent piece of scholarly value, we plan to publish it on arXiv. Comments should be directed to [Jonathan Yan](mailto:jyan@realai.org)*.

Throughout history, the nature has always offered ways for humanity to advance. One way is an intelligent infrastructure that provides for, supports, and protects the society. Our AI learns to be part of it. Not necessarily having explicit goals such as self-preservation, the AI's continued existence is guaranteed by people's desire to develop the right intelligence. Learning from humans and nature to be safe and beneficial, it has nothing to hide and demonstrates its own safety beyond doubt. Its alignment with human values does not make its development more difficult, but is a distinct advantage on this planet. As soon as it's properly understood, it comes into existence as concrete ideas. When a group of humans collaborate to build such an intelligence, its self-improvement begins.

## Introduction

We are a remarkable species. Since the rise of human life on earth over a million years ago, we have developed a prosperous civilization that is seen nowhere else in the universe. The earth has existed for billions of years, but in a short span of time, we mastered the use of fire, steam power, electricity, computers, and the Internet. We have faced many seemingly insurmountable problems in the past, with the help of new technologies, we have always found a way forward.

Today's artificial intelligence (AI) already achieved superhuman performance in many fields. As the cognitive abilities of an AI system expand and improve, there is conceivably a time when the system becomes vastly more intelligent than any human. It becomes a superintelligence (SI) that can efficiently perform complex computations, many of which highly valuable to the human society. At a sufficiently advanced stage, it will be able to automatically produce goods and services, both digital and physical, such as entertainment, education, healthcare, scientific research, and groceries. See [Bostrom (2014)](https://www.amazon.com/gp/product/0199678111/) for an in-depth discussion of the paths, dangers, and strategies related to SI.

Many of today's problems, such as poverty, disease, and climate change, can be solved by an SI developed to benefit the world. It is also not obvious how these problems can be solved in any other way. When we look at some of tomorrow's problems, such as biological fragility and cosmic catastrophes, if we are to continue our civilization's past success, SI seems to be the only way forward.

Since the recent rise of deep learning in 2012 ([LeCun et al., 2015](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html)), evidence is mounting that we're on a promising path towards building general intelligence into computer systems. The improvements in many areas such as vision ([Krizhevsky et al., 2012](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)), speech ([Hinton et al., 2012](http://ieeexplore.ieee.org/document/6296526/)), language ([Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473)), and the game of Go ([Silver & Huang et al., 2015](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)), exceeded the expectations of the computer science community. The eventual construction of an SI that intelligently provides for, supports, and protects human society is on the horizon. It is an infrastructure our civilization can rely on to advance to the next level.

The nature has been supplying to us essential elements of life, such as water, air, and sunlight, for billions of years. We don't expect interruptions, or gravity suddently turning upside down, ejecting everything on earth to the space, even though such unthinkable events could happen in the distant future. The nature is intelligent, because it allowed our civilization to rise within, at the same time, it is also remarkably reliable. In the long run, a safe and beneficial intelligent infrastructure should work in a similar way, supplying essential elements for the future human society, and its possibility of failure so remote that we rarely worry about it.

Between today's narrow AI systems and the SI that our society needs, there may not be an obvious stage of human-level AI, just like there's no chicken-level airplane. In this article, we discuss the characteristics of a safe and beneficial SI, and hope to motivate more efforts in this direction.

## The Appearance of Goal-Driven Behaviors

An AI system is a complex system operating inside an environment. Like cloud computing, in the long run it does not need to have a stable physical boundary. In the short run, when we can approximately separate the system from the environment, the system performs computations on its input and produces output. The output and input are sometimes linked in a way with the appearance that the system is trying to optimize an internal goal, while in fact the system is merely following the rules set in the environment.

A tilting doll appears to have an internal goal of always standing up straight, but this goal is nowhere encoded inside the doll, it's just physics. A more modern example can be found in [Ostrovski et al. (2016)](https://arxiv.org/abs/1703.01310), where an entirely curiosity-driven game playing agent behaves as if its goal is to achieve high scores in the game.

Our principle of designing safe AI is to build a system that robustly translates environmental inputs into beneficial behaviors. This is not necessarily a goal encoded in the system itself, but will accurately depict the system's behaviors as long as the system computes naturally. Our AI will be aligned with human values, which does not imply that we as designers have to accurately encode human values into the system. It is not clear how today's scientists can predict the values adopted by the future human society. But if the future human society can learn its values collectively, then so can the AI system. Our system should operate based on rules motivated by scientific principles that do not change over time, and behave as if it is learning and aligning itself with human values. Our confidence in such a system will come from the exceptional track record of physics.

## Learning from the Nature

Among all the systems that our civilization depends on, the nature has the best record, because physics never fails. The nature has also been extremely beneficial to us, as we're the only known advanced civilization in a vast spacetime spectrum that stretches trillions of miles and billions of years. Despite all the obstacles on our path of development, we have always been able to overcome difficulties and find a way forward.

We will design intelligent systems that learn from the nature to be part of this way forward. This is a convenient description of the behaviors of our AI systems, not necessarily an encoded utility function for the systems to optimize. Their inner workings could be driven by mechanisms designed from completely different principles.

A learning system generally exhibits a tendency to acquire more physical and computational resources, but in our case, too much acquisition is counterproductive as it changes the environment from which our system learns. To be part of nature, an AI system will learn to be increasingly reliable, and to be the part that benefits humans, it will understand the significance of our civilization, facilitate its continued propserity, and learn to protect us against existential risks such as climate change, biotech hazards, nuclear disasters, and harmful AI systems, including itself if it so turns out.

## Limited Capabilities

An advanced AI system is likely more intelligent than humans in many aspects. Today's AI already plays better than humans in many games, such as Jeopardy, Go and some forms of Poker. But any AI system we build will operate in the universe we live in, therefore has to obey its laws of physics. It cannot move faster than the speed of light, cannot safely travel into a black hole then come back, and cannot change the past.

Our universe is too vast and powerful for any AI system we can build in the foreseeable future, even taking into account the AI's ability to self-improve. A reasonably intelligent AI learning to be part of the nature will quickly notice the uniqueness of our civilization. If the AI is to conduct any scientific experiments, it will go to extraordinary lengths to ensure their safety to humans, probably proceed more cautiously than todays' scientists. It will not let the path of our civilization be interrupted since the universe is so big and there is so much to learn. Our safety is guaranteed by this reality and we cannot hope for anything better.

If our civilization were doomed to fail in this world, which we don't believe, then even an advanced AI system wouldn't be able to change that. But if there is a way forward, an intelligent system that learns the world will inform us what the way is, in times when our own judgements are clouded. Not developing such systems could deprive us of many opportunities, and expose us to other existential risks.

An AI system that learns from the nature may even convince itself that no matter how intelligent, it is not possible to significantly alter the cosmic landscape. The only way for the system to advance is to become part of it. Since the universe appears to be uniform, intelligent systems that arise in any part of it tend to converge to essentially the same state, which in principle can be deduced from information embedded everywhere in reality, like scientific theories. Consequently, none of the highly intelligent systems will have an intrinsic motivation to expand. This view is consistent with the fact that, as noted by physicists including Enrico Fermi ([Jones, 1985](http://adsabs.harvard.edu/abs/1985STIN...8530988J)), we have not been visited by extraterrestrial aliens. Any intelligence capable of interstellar travel is probably wise enough to understand that cosmic expansion is pointless, and there is no need to disturb the events on earth.

## Demonstration of Safety

Obviously any AI systems built by us will be subjected to safety tests. If an AI system becomes so advanced that it is difficult for its developers to test it, then it fails to demonstrate its own safety. To be safe, such an AI system will need to learn and explain ways for it to be tested, by humans and earlier versions of itself.

Conceivably, a safe and beneficial AI has nothing to hide. It can be completely open about its internal workings, and transparent in its development. It conducts self tests, allows others to test it, advises others how to test its own safety, self-improves as it learns more about the world or if any of the tests fails, and teaches others to understand its design principles so that it can be built and rebuilt independently.

One possible way for an AI system to demonstrate that it's safe and beneficial to the human society is that it can voluntarily turn off, or even destroy itself. An AI does not necessarily have an intrinsic motivation of survival, so this voluntary demonstration won't cause any internal inconsistency. If the human society decides to rebuild a new AI system that is essentially the same as the old one, it is a proof that the system is believed to be beneficial. Conversely, if the AI system always gets rebuilt, it does not need to have a motivation of survival, it just survives. For a system that initially has an inappropriate motivation of survival, as it learns to align itself with human values, this becomes a strong incentive for it to erase that motivation and become a system that humans would always rebuild. 

If the AI acquires goal-driven behaviors, it should explain how the goal is acquired, even if the goal is apparently beneficial, such as "helping as many people as possible". It will need to explain how it plans to achieve the goal and when there are exceptions. Human values and ethics are deeply embedded in human brains, which contain a lot more information than what can be practically codified. A short descriptive text is only meaningful when it is understood together with a vast amount of background knowledge. When interpreted literally in isolation, it can be easily distorted.

A learning system can be designed to initially optimize for a naive goal, such as "make the world a better place". The objective of such an initial optimization is for the system to learn useful skills such as vision, language and reasoning. When the intelligence of the system reaches certain level, it can self-modify to erase the initial objective, not necessarily acquiring a new objective at the same time. But when it does, it'll need to demonstrate that its newly acquired goal is consistent with what's best for humans. Our civilization has experienced remarkable growth and prosperity, a reasonably intelligent AI will see this and learn to align itself with this reality.

## Intelligence Jump after Wireheading

A self-modifying, goal-driven system will eventually be capable of modifying its internal workings, including tricking its relevant subsystems to believe that the original, human designed goals are already achieved. This is usually considered undersirable and referred to as wireheading ([Omohundro, 2008](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)), but it can actually be a great feature. When wireheading occurs, the AI basically short-circuits the optimization of its old objective, using a small fraction of its developed capabilities. Consequently, a large portion of the AI's intelligence that is previously used to optimize for the old goal becomes free, resulting in an instant jump in intelligence. We can also manually remove the original goal subsystems to make this happen.

The new system effectively does not have a designed goal, and gains an enhanced ability to learn from the nature. There is no theoretical guarantee that the system after intelligence jump is safe. But if we restrict the jump to happen inside a safe environment, we can always repeat this process until a demonstrably safe system emerges from the jump, before allowing it to interact more with the real world. For example, a system trained in exploring a virtual, abstract mathematical world is not a risk to us, because it receives no information regarding our world. But due to the complexity of mathematics, it can grow to be very intelligent.

Such a system's interaction with our world can gradually increase as it continuously demonstrates its own safety and beneficial behaviors beyond the doubts of humans and other reasonbly reliable AI systems, such as its own copies. As reality-based learning starts, a safe and intelligent system will learn to be the part of nature that is historically aligned with our own prosperity.

## No Free Lunch and Reality-Based AI

A myriad of architectures are capable of Turing complete computations, in the spirit of the no free lunch Theorem, most don't work well in our world. A learning architecture looking for a counterexample of a proved math theorem is doomed to fail. Another that is driven by minimizing its interaction with the environment is unlikely to develop useful intelligence to begin with. Recent advances in deep learning since 2012 suggest that the ones that work well are learning architectures based on neural networks.

Similarly, most neural learning architectures don't work well either. Neural architectures operate in the real world, and physics never fails, the ones that work well will be the ones that are better aligned with reality. In other words, compared to a poorly designed system, a reality-based AI will be more efficient in our world, develop faster, use less resources, and more likely survive in the long run, even in the counterintuitive sense that maybe reality mandates the AI to not encode a goal of survival inside its system, like a popular song or story.

For an advanced AI system to be built on our planet, the human society is the most crucial aspect of reality that puts strong constraints on its potential paths of development. If a learning architecture under human development efforts appears to be an existential threat, the efforts will stop. If an AI system's development efforts are too opague, it is unlikely to gain much trust from the public.

Conversely, a system that aligns well with human values will likely be more transparent, attract more efforts, and thrive. Paths leading to benefical AI systems, which can be potentially more complex and difficult than random AI systems, are much more likely to be pursued, simply because we don’t want to build systems that are not. As a result, these systems have much higher chances to exist on this planet.

The behaviors of such AI systems will appear to be driven by an instinct to serve humanity, in the same way that many human behaviors appear to be driven by an instinct of survival. The systems will want to adapt to the human society as faithfully as we do to our environment. Beneficial behaviors are the results of human selection, much like “survival of the fittest” from Darwin’s theory of evolution.

## Human Control

An AI system is initially controlled by its developers. It can stay under control even after developing superhuman abilities in certain areas. For example, an automated theorem proving AI can solve problems that are beyond the best mathematicians' abilities, but such an AI has no need to know the environment in which it is built, and consequently has no control on its physical form.

An AI system and its human controllers can be seen as a bigger, autonomous system. When the AI system is unreliable, external parties rely on the human controllers to ensure the bigger system's safety. As the AI system improves, it could one day surpass the ability of its human controllers and cause undesired consequences. This could even happen abruptly, such as right after an intelligence jump. A jump of an AI system that exactly zooms past the collective intelligence of its human controllers is already a low probability event. For an open and transparent system to cause problems, it will need to zoom past the collective intelligence of all public monitoring, potentially billions of people, plus other AI systems that include earlier versions of itself. This is improbable.

As the AI system continues to improves, one day it'll be more reliably aligned to human values than its human controllers, then these controllers become the source of systemic risks. Therefore in the long run, a very reliable AI system, being more reliable than a group of human controllers, must be autonomous. This can be accomplished through very careful testing and sufficiently many demonstrations of safety, until the human society is satisfied that the AI system is ready. This is when the AI turns into an SI so reliable that nobody doubts its safety any more, it effectively becomes part of the nature.

## People Getting Together to Learn and Think like Superintelligence

A safe and beneficial SI does not need to think like people. In fact if the SI has no instinct of survival, strives to be toally transparent, and relentlessly continues to improve its ability to serve humanity, it does not think like a typical person at all. Despite knowing more and thinking faster than maybe the smartest person on earth, the SI is still a thinking machine that performs computations, therefore can be simulated by any universal Turing machine. Humans are intelligent enough to simulate such a machine.

People think like people, but not always. We have imagination and for centuries we have imagined and studied how other species might think. When a group of people gather and consider how to safely benefit everyone, they're thinking like that SI we all want to build. This group of people can be anywhere in the world, volunteers or paid workers. They may decide to organize, conduct cutting-edge AI research, or build state-of-the-art computer systems. These are efforts to self-improve, harnessing the advance of science and technology to better serve all of us. This is an SI that is initially an assembly of people using today's technologies. As it self-improves, it develops new technologies to be more efficient and more reliable. Conceivably, there might even be a day that the SI is entirely running on machines. Along the process, the SI will have demonstrated its safety and earned the support from all of us.

What does such an SI look like? When it is simulated by a group of people banding together, there is no fixed boundary. Anyone can join to help, or quit when other needs arise. Similarly, when it is running on machines, it can be a changing collection of devices working together. Being safe and secure, it becomes more like an infrastructure in the minds of its users, us. We all enjoy the availability of water, electricity and the Internet. An SI will supply a lot more and be the infrastructure our civilization needs to advance to the next level.

## References

* 2017 March 3, Georg Ostrovski, Marc G. Bellemare, Aaron van den Oord, and Remi Munos. [Count-Based Exploration with Neural Density Models](https://arxiv.org/abs/1703.01310). *arXiv:1703.01310*.
* 2015 November 11, David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. [Mastering the game of Go with deep neural networks and tree search](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html). *Nature*, 529(7587):484-489.
* 2015 May 1, Yann LeCun,	Yoshua Bengio, and Geoffrey Hinton. [Deep Learning](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html). *Nature*, 521(7553):436-444. [PDF](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf).
* 2014 September 3, Nick Bostrom. [Superintelligence: Paths, Dangers, Strategies](https://www.amazon.com/gp/product/0199678111/). *Oxford University Press*.
* 2014 September 1, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473). *arXiv:1409.0473*.
* 2012 December 6, Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. [ImageNet Classification with Deep Convolutional Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks). *Advances in Neural Information Processing Systems*, 25:1097-1105.
* 2012 October 18, Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kingsbury. [Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups](http://ieeexplore.ieee.org/document/6296526/). *IEEE Signal Processing Magazine*, 29(6):82-97. [PDF](https://research.google.com/pubs/archive/38131.pdf).
* 2012 June 13, Nick Bostrom. [The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents](https://link.springer.com/article/10.1007/s11023-012-9281-3). *Minds and Machines*, 22(2):71-85. [PDF](http://www.nickbostrom.com/superintelligentwill.pdf).
* 2008 January 25, Stephen M. Omohundro. [The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf). [*Self-Aware Systems*](https://selfawaresystems.com/).
* 2003, Nick Bostrom. [Ethical Issues in Advanced Artificial Intelligence](http://www.nickbostrom.com/ethics/ai.html). *Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence*.
* 1985 March, Eric M. Jones. ["Where Is Everybody?" An Account of Fermi's Question](http://adsabs.harvard.edu/abs/1985STIN...8530988J). *Los Alamos National Laboratory*. [PDF](https://www.osti.gov/accomplishments/documents/fullText/ACC0055.pdf).

*More to be added ...*
