---
permalink: /frontier/safety.html
---
# Safety

On this page, we consider how to ensure that the AI system we develop will benefit the whole world.

## Timeline of References

### News

* 2017 January 5-8, The [Future of Life Institute](https://futureoflife.org/) brought together a group of AI researchers and thought leaders for the [Beneficial AI 2017](https://futureoflife.org/bai-2017/) conference that developed [The Asilomar AI Principles](https://futureoflife.org/ai-principles/).
* 2015 January 28, Bill Gates wrote that he is "concerned about super intelligence". [*BBC*](http://www.bbc.com/news/31047780), [*Reddit*](https://www.reddit.com/r/IAmA/comments/2tzjp7/hi_reddit_im_bill_gates_and_im_back_for_my_third/co3r3g8/).
* 2015 January 12, A group of scientists and entrepreneurs, including Elon Musk and Stephen Hawking, signed an open letter promising to ensure AI research benefits humanity. [*Daily Mail*](http://www.dailymail.co.uk/sciencetech/article-2907069/Don-t-let-AI-jobs-kill-Stephen-Hawking-Elon-Musk-sign-open-letter-warning-robot-uprising.html).
* 2014 December 2, Prof. Stephen Hawking told the BBC that the development of full AI "could spell the end of the human race". [*BBC*](http://www.bbc.com/news/technology-30290540).
* 2014 October 27, Elon Musk said AI is our "biggest existential threat". [*The Guardian*](https://www.theguardian.com/technology/2014/oct/27/elon-musk-artificial-intelligence-ai-biggest-existential-threat).

### Research

* 2016 June 21, Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man√©. [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565). *arXiv:1606.06565*.
* 2015 January 12, Stuart Russell, Daniel Dewey, and Max Tegmark. [Research Priorities for Robust and Beneficial Artificial Intelligence](https://futureoflife.org/data/documents/research_priorities.pdf). *Future of Life Institute*.

## At Real AI

Among all the systems our civilization depends on, the nature has the best record: Physics never fails. We're the only known advanced civilization in vast time and space that stretch trillions of miles and billions of years. We have always been able to find a way forward.

We will design our intelligent system to learn from the nature, understand the significance of our civilization, and facilitate its continued propserity.

In order to achieve that, our AI will not be a naively utility-maximizing agent. Its learning mechanism is not necessarily a set of rules designed from optimizing a particular function. It will learn to be the way forward, protecting us against existential risks such as climate change, biotech, nuclear disaster, and harmful AI systems, including itself if it so turns out.
