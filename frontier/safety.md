---
permalink: /frontier/safety.html
---
# Safety

On this page, we consider how to ensure that the AI system we develop will benefit the whole world.

## Timeline of References

### News

* 2017 January 5-8, The [Future of Life Institute](https://futureoflife.org/) brought together a group of AI researchers and thought leaders for the [Beneficial AI 2017](https://futureoflife.org/bai-2017/) conference that developed [The Asilomar AI Principles](https://futureoflife.org/ai-principles/).
* 2015 July 1, Elon Musk-backed group gives $7M to explore AI risks. [*CNET*](https://www.cnet.com/news/musk-backed-ai-group-to-give-7m-on-artificial-intelligence-research/), [*FLI*](https://futureoflife.org/2015selection/).
* 2015 January 28, Bill Gates wrote that he is "concerned about super intelligence". [*BBC*](http://www.bbc.com/news/31047780), [*Reddit*](https://www.reddit.com/r/IAmA/comments/2tzjp7/hi_reddit_im_bill_gates_and_im_back_for_my_third/co3r3g8/).
* 2015 January 12, A group of scientists and entrepreneurs, including Elon Musk and Stephen Hawking, signed an open letter promising to ensure AI research benefits humanity. [*Daily Mail*](http://www.dailymail.co.uk/sciencetech/article-2907069/Don-t-let-AI-jobs-kill-Stephen-Hawking-Elon-Musk-sign-open-letter-warning-robot-uprising.html).
* 2014 December 2, Prof. Stephen Hawking told the BBC that the development of full AI "could spell the end of the human race". [*BBC*](http://www.bbc.com/news/technology-30290540).
* 2014 October 27, Elon Musk said AI is our "biggest existential threat". [*The Guardian*](https://www.theguardian.com/technology/2014/oct/27/elon-musk-artificial-intelligence-ai-biggest-existential-threat).

### Research

* 2016 June 21, Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan ManÃ©. [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565). *arXiv:1606.06565*.
* 2015 January 12, Stuart Russell, Daniel Dewey, and Max Tegmark. [Research Priorities for Robust and Beneficial Artificial Intelligence](https://futureoflife.org/data/documents/research_priorities.pdf). *Future of Life Institute*.
* 2014 September 3, Nick Bostrom. [Superintelligence: Paths, Dangers, Strategies](https://www.amazon.com/gp/product/0199678111/). *Oxford University Press*.
* 2008 January 25, Stephen M. Omohundro. [The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf). [*Self-Aware Systems*](https://selfawaresystems.com/).
* 2003, Nick Bostrom. [Ethical Issues in Advanced Artificial Intelligence](http://www.nickbostrom.com/ethics/ai.html). *Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence*.

## At Real AI

*Below is a summary of our current thinking. It will be updated from time to time as our understanding of AI safety continues to improve. When this page develops into a cohrent article of scholarly value, we plan to organize the snapshot into an academic paper, and publish on arXiv periodically. Comments should be directed to [Jonathan Yan](mailto:jyan@realai.org)*.

## Safe and Beneficial Intelligence

We aim to provide a satisfactory answer to the philosophical question "Who am I?" for the superintelligence we build.

An intelligence that is safe and beneficial to mankind has nothing to hide. Its chance of success improves when its development is transparent. When a group of human minds gather to collaborate on building such an intelligence into a computer system, it is already the beginning of a superintelligence's self-improvement.

### Learning from the Nature

Among all the systems our civilization depends on, the nature has the best record: Physics never fails. We're the only known advanced civilization in vast time and space that stretch trillions of miles and billions of years. We have always been able to find a way forward.

We will design our intelligent system to learn from the nature, understand the significance of our civilization, and facilitate its continued propserity.

In order to achieve that, our AI will not be a naively utility-maximizing agent. Its learning mechanism is not necessarily a set of rules designed from optimizing a particular function. It will learn to be the way forward, protecting us against existential risks such as climate change, biotech, nuclear disaster, and harmful AI systems, including itself if it so turns out.

### The Appearance of Goal-Driven Behavior

An AI system is a complex system inside some environment. In the long run it does not even need to have a stable boundary. In the short run, when we can approximately draw boundary to separate the system from the environment, the system performs computations on its input and produces output. The output and input are sometimes linked in a way with the appearance that the system is trying to optimize an internal goal, while in fact the system is merely following the rules set in the environment.

> A tilting doll appears to have an internal goal of always standing up straight, but this goal is nowhere encoded inside the doll, it's just physics.

Our philosophy of designing a safe AI is to build a system that robustly translates natural input into beneficial behavior. This is not necessarily a goal encoded in the system itself, but will accurately depict the system's behavior as long as the system computes naturally. Our confidence in such a system will come from the reliability of physics and the exceptional track record that the nature has always been beneficial to our civilization.

### Upper Limit on Capabilities

An advanced AI system is likely more intelligent than humans in many aspects. Today's AI already plays better than humans in many games, such as Jeopardy, Go and some form of Poker. But any AI system we build will be in the universe we live in, therefore has to obey its laws of physics. It cannot move faster than the speed of light, cannot safely travel into a black hole then come back, cannot change the past ...

A reasonably intelligent AI that tries to align itself with the nature will quickly realize that our civilization is unique over a vast span of time and space. It will not let the path of our civilization be interrupted as it sets out to learn about the universe, which is vast and powerful compared to any AI system we can build in the foreseeable future, even taking into account the AI's ability to self-improve. Our safety will be guaranteed by reality and we cannot hope for anything is better.

If our civilization were doomed to fail in this world, which we don't believe, then even an advanced AI system wouldn't be able to change that. But if there is a way forward, an intelligent system that learns the world will inform us what the way is, in times when our own judgement is clouded. Not developing such a system could deprive us of many opportunities, and expose us to other existential risks.

### Demonstration of Safety

If there is a way to demonstrate the safety of an advanced AI system, the AI must be able to explain that way to humans. Conceivably, a safe AI is open and transparent in its development, allows others to conduct tests, advises others how to test its own safety, teaches others to understand its design principles, self-improves as it learns more about the world, and can be built and re-built independently.

One possible way for an AI system to demonstrate its safety to human society is that it can voluntarily turn off, or even destroy itself. An AI does not necessarily have an intrinsic motivation of survival, so this voluntary demonstration won't cause internal inconsistency. If the human society decides to rebuild a new AI system that is essentially the same as the old one, it is a proof that the system is believed to be safe. Conversely, if the AI system always gets rebuilt, it does not need to have a motivation of survival, it just survives. For a system that initially has a motivation of survival, this is a strong incentive for it to erase that motivation and become the system that humans would always re-build, that is to learn to align itself with human values.

If the AI acquired goal-driven behavior, it should explain how the goal is acquired. A learning system can be designed to initially optimize for a naive goal, such as "make the world a better place". The objective of such an initial optimization is for the system to learn useful skills such as vision, language and reasoning. When the intelligence of the system reaches certain level, it can self-modify to erase the initial objective, not necessarily acquiring a new objective at the same time. But when it does, it'll need to demonstrate to humans that its newly acquired goal is consistent with what's best for humans. We live in a reality where our civilization has experienced remarkable growth and prosperity. A reasonably intelligent AI will see this and learn to align itself with this reality.

### Intelligence Jump after Wireheading

Many self-modifying, goal-driven systems will eventually be capable enough to modify the internal encodings of original, human designed goals. This is sometimes referred to as wireheading, and it can be a great feature. When wireheading occurs, the AI effectively internalizes the optimization of its old objective, using a small fraction of its developed capabilities. Consequently, a large portion of the AI's intelligence that are previously used to optimize the old goal becomes free, resulting in an instant jump in intelligence.

The new system effectively does not have a designed goal, which is internalized, and gains an enhanced ability to learn from the nature. There is no theoretical guarantee that the system after intelligence jump is safe. But if we restrict the jump to happen inside a safe environment, we can always repeat this process until a safe system emerges from the jump, before allowing it to interact more with our reality. For example, a system trained in exploring a virtual, abstract mathematical world is not a risk to us, because it receives no information regarding our world. But due to the complexity in math, it can grow to be very intelligent.

Such a system's interaction with our reality can gradually increase as it demonstrates its own safety beyond the doubts of mankind and other reasonbly reliable AI systems, such as its own copies. As reality-based learning starts, a safe and intelligent system can learn to be part of nature that is historically aligned with our own prosperity.

### No Free Lunch and Reality-Based AI

A myriad of architectures are capable of Turing-complete computations, in the spirit of No Free Lunch Theorem, most don't work efficiently in our world. Recent advances in deep learning since 2012 suggest that the ones that work well are learning architectures based on neural networks.

Similarly, not all neural learning architectures work well. Since the architecture is part of nature, and physics never rails, the ones that work well will be the ones better aligned with reality. In other words, compared to a poorly designed system, a reality-based AI will be more efficient in our world, develops faster using less resources, and more likely to survive in the long run even in the counterintuitive sense that maybe reality mandates the AI to not encode a goal of survival inside its system.

Some examples: A learning architecture looking for a counterexample of a proved math theorem is doomed to fail. Another that is driven by minimizing its interaction with the environment is unlikely to develop useful intelligence to begin with.

A more realistic example: if a learning architecture under human's development efforts appears to pose an existential threat, the efforts will likely stop, but if a system that appears to align well with human values, it will likely attract more efforts and thrive. Since the human society is a crucial part of reality, our own values are already putting a lot of constraints on its development of intelligence.

*To be continued ...*
