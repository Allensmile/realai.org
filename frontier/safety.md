---
permalink: /frontier/safety.html
---
# Safety

On this page, we consider how to ensure that the AI system we develop will benefit the whole world.

## Timeline

### News

* 2017 January 5-8, The [Future of Life Institute](https://futureoflife.org/) brought together a group of AI researchers and thought leaders for the [Beneficial AI 2017](https://futureoflife.org/bai-2017/) conference that developed [The Asilomar AI Principles](https://futureoflife.org/ai-principles/).
* 2015 July 1, Elon Musk-backed group gives $7M to explore AI risks. [*CNET*](https://www.cnet.com/news/musk-backed-ai-group-to-give-7m-on-artificial-intelligence-research/), [*FLI*](https://futureoflife.org/2015selection/).
* 2015 January 28, Bill Gates wrote that he is "concerned about super intelligence". [*BBC*](http://www.bbc.com/news/31047780), [*Reddit*](https://www.reddit.com/r/IAmA/comments/2tzjp7/hi_reddit_im_bill_gates_and_im_back_for_my_third/co3r3g8/).
* 2015 January 12, A group of scientists and entrepreneurs, including Elon Musk and Stephen Hawking, signed an open letter promising to ensure AI research benefits humanity. [*Daily Mail*](http://www.dailymail.co.uk/sciencetech/article-2907069/Don-t-let-AI-jobs-kill-Stephen-Hawking-Elon-Musk-sign-open-letter-warning-robot-uprising.html).
* 2014 December 2, Prof. Stephen Hawking told the BBC that the development of full AI "could spell the end of the human race". [*BBC*](http://www.bbc.com/news/technology-30290540).
* 2014 October 27, Elon Musk said AI is our "biggest existential threat". [*The Guardian*](https://www.theguardian.com/technology/2014/oct/27/elon-musk-artificial-intelligence-ai-biggest-existential-threat).

### Research

* 2016 June 21, Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565). *arXiv:1606.06565*.
* 2015 January 12, Stuart Russell, Daniel Dewey, and Max Tegmark. [Research Priorities for Robust and Beneficial Artificial Intelligence](https://futureoflife.org/data/documents/research_priorities.pdf). *Future of Life Institute*.
* 2014 September 3, Nick Bostrom. [Superintelligence: Paths, Dangers, Strategies](https://www.amazon.com/gp/product/0199678111/). *Oxford University Press*.

## At Real AI

*Below is a summary of our current thinking. It will be revised from time to time as our understanding of AI safety improves. When this page develops into a cohrent article of scholarly value, we plan to organize the snapshot into an academic paper, and publish on arXiv periodically. Comments should be directed to [Jonathan Yan](mailto:jyan@realai.org)*.

## Towards A Safe and Beneficial Intelligence

We attempt to answer the philosophical question “Who am I?” for the superintelligence (SI) to be built. Throughout history, the nature has always offered a way for humanity to advance. Our SI learns to be this way, an infrastructure that provides for, supports, and protects the society. Not necessarily having explicit goals such as self-preservation, its continued existence is guaranteed by people's desire to build the right intelligence. Learning from humans and nature to be safe and beneficial, it has nothing to hide and demonstrates its own safety beyond doubt. Its alignment with human values does not make its development more difficult, but is a distinct advantage on this planet. When a group of human minds collaborate to build such an intelligence, the SI’s self-improvement begins.

### The Way Forward

We are a remarkable species. Since the rise of human life on earth over a million years ago, we have developed a prosperous civilization that is seen nowhere else in the universe. The earth has existed for billions of years, but in a short span of time, we mastered the use of fire, steam power, electricity, computers, and the Internet. We have faced many seemingly insurmountable problems in the past, with the help of new technologies, we have always found a way forward.

Today's artificial intelligence (AI) already achieved superhuman performance in many fields. As the cognitive abilities of an AI system expand and improve, there is conceivably a time when the system becomes vastly more intelligent than any human. SI is a system that efficiently performs complex computations, many of them highly valuable to human society. At sufficiently advanced stage, it will be able to automatically produce goods and services, both digital and physical, such as entertainment, education, healthcare, scientific research, and grocery.

Many of today's problems, such as poverty, disease, and global warming, can be solved by an SI developed to benefit the world. It is also not obvious how these problems can be solved in any other way. When we look at some of tomorrow's problems, such as biological fragility and cosmic catastrophes, if we were to continue our civilization's past success, SI seems to be the only way forward.

Since the recent rise of deep learning in 2012, evidence is mounting that we're on a promising path towards building general intelligence into computer systems. The improvements in many areas such as vision, speech, language, and the game of Go, exceeded the expectations of the computer science community. The eventual construction of an SI that intelligently provides for, supports, and protects human society is on the horizon. It is an infrastructure our civilization can rely on to advance to the next level.

The nature has been supplying to us essential elements of life, such as water, air, and sunlight, for billions of year. We don't expect interruption, or gravity suddently turning upside down, ejecting everything on earth to the space. Such unthinkable events could happen, maybe in billions of years. The nature is intelligent, because it allowed our civilization to rise within, at the same time, it is also remarkably reliable. In the long run, a safe and beneficial intelligent infrastructure should work in a similar sense, supplying essential elements for future human society. Its possibility of failure is so remote that we rarely worry about it.

Between today's narrow AI systems and the SI that our society needs, there may not be an obvious stage of human-level AI, just like there's no chicken-level airplane. In this article, we discuss the characteristics of a safe and beneficial SI, and hope to motivate more efforts in this direction.

### The Appearance of Goal-Driven Behavior

An AI system is a complex system inside some environment. In the long run it does not even need to have a stable boundary. In the short run, when we can approximately draw boundary to separate the system from the environment, the system performs computations on its input and produces output. The output and input are sometimes linked in a way with the appearance that the system is trying to optimize an internal goal, while in fact the system is merely following the rules set in the environment.

A tilting doll appears to have an internal goal of always standing up straight, but this goal is nowhere encoded inside the doll, it's just physics. A more modern example can be found in [Ostrovski et al. (2016)](https://arxiv.org/abs/1703.01310), where an entirely curiosity-driven game playing agent behaves as if its goal is to achieve high scores in the game.

Our principle of designing a safe AI is to build a system that robustly translates environmental inputs into beneficial behaviors. This is not necessarily a goal encoded in the system itself, but will accurately depict the system's behavior as long as the system computes naturally. Our AI will be aligned with human values, this does not imply we as designers have to accurately encode human values into the system. It is not clear how today's scientists can predict the values adopted by a future human society. But if the future human society can learn its values collectively, then so can the AI system. Our system should operate based on rules motivated by scientific principles that do not change over time, and behave as if it is learning and aligning itself with human values. Our confidence in such a system will come from the exceptional track record of physics.

### Learning from the Nature

Among all the systems our civilization depends on, the nature has the best record: Physics never fails. We're the only known advanced civilization in vast time and space that stretch trillions of miles and billions of years. We have always been able to find a way forward.

We will design our intelligent system to learn from the nature, understand the significance of our civilization, and facilitate its continued propserity.

In order to achieve that, our AI will not be a naively utility-maximizing agent. Its learning mechanism is not necessarily a set of rules designed from optimizing a particular function. It will learn to be the way forward, protecting us against existential risks such as climate change, biotech, nuclear disaster, and harmful AI systems, including itself if it so turns out.

### Upper Limit on Capabilities

An advanced AI system is likely more intelligent than humans in many aspects. Today's AI already plays better than humans in many games, such as Jeopardy, Go and some form of Poker. But any AI system we build will be in the universe we live in, therefore has to obey its laws of physics. It cannot move faster than the speed of light, cannot safely travel into a black hole then come back, cannot change the past ...

A reasonably intelligent AI that tries to align itself with the nature will quickly realize that our civilization is unique over a vast span of time and space. It will not let the path of our civilization be interrupted as it sets out to learn about the universe, which is vast and powerful compared to any AI system we can build in the foreseeable future, even taking into account the AI's ability to self-improve. Our safety will be guaranteed by reality and we cannot hope for anything is better.

If our civilization were doomed to fail in this world, which we don't believe, then even an advanced AI system wouldn't be able to change that. But if there is a way forward, an intelligent system that learns the world will inform us what the way is, in times when our own judgement is clouded. Not developing such a system could deprive us of many opportunities, and expose us to other existential risks.

An AI system that learns from the nature may even convince itself that no matter how intelligent, it is not possible to significantly alter the cosmic landscape. The only way for the system to develop is to be part of nature. Since the nature appears to be uniform everywhere, intelligent systems that arise anywhere in the universe tend to converge to essentially the same intelligence, thus none of them will have an intrinsic motivation to expand. This view is supported by the fact, as noted by physicists including Enrico Fermi, that we have not been visited by extraterrestrial aliens. Any intelligence capable of interstellar travel is probably wise enough to understand that there is no need to disturb the current path on earth.

### Demonstration of Safety

If there is a way to demonstrate the safety of an advanced AI system, the AI must be able to explain that way to humans. Conceivably, a safe AI is open and transparent in its development, allows others to conduct tests, advises others how to test its own safety, teaches others to understand its design principles, self-improves as it learns more about the world, and can be built and re-built independently.

One possible way for an AI system to demonstrate its safety to human society is that it can voluntarily turn off, or even destroy itself. An AI does not necessarily have an intrinsic motivation of survival, so this voluntary demonstration won't cause internal inconsistency. If the human society decides to rebuild a new AI system that is essentially the same as the old one, it is a proof that the system is believed to be safe. Conversely, if the AI system always gets rebuilt, it does not need to have a motivation of survival, it just survives. For a system that initially has a motivation of survival, this is a strong incentive for it to erase that motivation and become the system that humans would always re-build, that is to learn to align itself with human values.

If the AI acquired goal-driven behavior, it should explain how the goal is acquired. A learning system can be designed to initially optimize for a naive goal, such as "make the world a better place". The objective of such an initial optimization is for the system to learn useful skills such as vision, language and reasoning. When the intelligence of the system reaches certain level, it can self-modify to erase the initial objective, not necessarily acquiring a new objective at the same time. But when it does, it'll need to demonstrate to humans that its newly acquired goal is consistent with what's best for humans. We live in a reality where our civilization has experienced remarkable growth and prosperity. A reasonably intelligent AI will see this and learn to align itself with this reality.

### Intelligence Jump after Wireheading

Many self-modifying, goal-driven systems will eventually be capable enough to modify the internal encodings of original, human designed goals. This is sometimes referred to as wireheading, and it can be a great feature. When wireheading occurs, the AI effectively internalizes the optimization of its old objective, using a small fraction of its developed capabilities. Consequently, a large portion of the AI's intelligence that are previously used to optimize the old goal becomes free, resulting in an instant jump in intelligence.

The new system effectively does not have a designed goal, which is internalized, and gains an enhanced ability to learn from the nature. There is no theoretical guarantee that the system after intelligence jump is safe. But if we restrict the jump to happen inside a safe environment, we can always repeat this process until a safe system emerges from the jump, before allowing it to interact more with our reality. For example, a system trained in exploring a virtual, abstract mathematical world is not a risk to us, because it receives no information regarding our world. But due to the complexity in math, it can grow to be very intelligent.

Such a system's interaction with our reality can gradually increase as it demonstrates its own safety beyond the doubts of mankind and other reasonbly reliable AI systems, such as its own copies. As reality-based learning starts, a safe and intelligent system can learn to be part of nature that is historically aligned with our own prosperity.

### No Free Lunch and Reality-Based AI

A myriad of architectures are capable of Turing-complete computations, in the spirit of No Free Lunch Theorem, most don't work efficiently in our world. Recent advances in deep learning since 2012 suggest that the ones that work well are learning architectures based on neural networks.

Similarly, not all neural learning architectures work well. Since the architecture is part of nature, and physics never rails, the ones that work well will be the ones better aligned with reality. In other words, compared to a poorly designed system, a reality-based AI will be more efficient in our world, develops faster using less resources, and more likely to survive in the long run even in the counterintuitive sense that maybe reality mandates the AI to not encode a goal of survival inside its system.

Some examples: A learning architecture looking for a counterexample of a proved math theorem is doomed to fail. Another that is driven by minimizing its interaction with the environment is unlikely to develop useful intelligence to begin with.

A more realistic example: if a learning architecture under human's development efforts appears to pose an existential threat, the efforts will likely stop, but if a system that appears to align well with human values, it will likely attract more efforts and thrive. Since the human society is a crucial part of reality, our own values are already putting a lot of constraints on its development of intelligence.

### Human Control

An AI system is intially controlled by its developers. It can stay under control even after developing superhuman abilities in certain tasks. For example, an automated theorem proving AI can solve problems that are beyond the best mathematicians' abilities, but such an AI has no need to know the environment in which it is built, and consequently has no control on its physical form.

An AI system and its human controllers can be seen as a bigger system that is autonomous. When the AI system is unreliable, external parties rely on the human controllers to ensure the bigger system's safety. As the AI system improves, one day it'll be more reliably aligned to human values than its human controllers, then these controllers become the source of systemic risk. Therefore in the long run, a very reliable AI system, being more reliable than a group of human controllers, must be autonomous. This can be accomplished through very careful testing and sufficiently many demonstrations of safety, until the human society is satisfied that the AI system is ready. Eventually if the AI system is so reliable that nobody doubts its safety, it effectively becomes part of nature.

### People Getting Together to Learn and Think like Superintelligence

A safe and beneficial SI does not need to think like people. In fact if the SI has no instinct of survival, strives to be toally transparent, and relentlessly continues to improve its ability to serve mankind, it does not think like a typical person at all. Despite knowing more and thinking faster than maybe the smartest person on earth, the SI is still a thinking machine that performs computations, therefore can be simulated by any universal Turing machine. Humans are already intelligent enough to simulate such a machine.

People think like people, but not always. We have imagination and for centuries we have imagined and studied how other species might think. When a group of people gather and consider how to safely benefit everyone, they're thinking like that SI we all want to build. This group of people may decide to organize, conduct cutting-edge AI research, build state-of-the-art computer systems. These are efforts to self-improve, harnessing the advance of science and technology to better serve all of us. This is an SI that is initially an assembly of people using today's technologies. As it self-improves, it develops new technologies to be more efficient and more reliable. Conceivably, there might even be a day that the SI is entirely running on machines. Along the process, the SI will have demonstrated its safety and earned the support of mankind.

What does such an SI look like? When it is simulated by a group of people banding together, there is no fixed boundary. Anyone can join to help, or quit when there are other needs. Similarly, when it is running on machines, it can be a changing collection of devices working together. Being safe and secure, it becomes more like an infrastructure in the minds of its users, us. We all enjoy the supply of water, electricity and Internet. An SI will supply a lot more and be the infrastructure our civilization needs to advance to the next level.

### References

* 2017 March 3, Georg Ostrovski, Marc G. Bellemare, Aaron van den Oord, and Remi Munos. [Count-Based Exploration with Neural Density Models](https://arxiv.org/abs/1703.01310). *arXiv:1703.01310*.
* 2008 January 25, Stephen M. Omohundro. [The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf). [*Self-Aware Systems*](https://selfawaresystems.com/).
* 2003, Nick Bostrom. [Ethical Issues in Advanced Artificial Intelligence](http://www.nickbostrom.com/ethics/ai.html). *Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence*.

*More to be added ...*
