---
permalink: /frontier/safety.html
---
# Safety

On this page, we consider how to ensure that the AI system we develop will benefit the whole world.

## Timeline of References

### News

* 2017 January 5-8, The [Future of Life Institute](https://futureoflife.org/) brought together a group of AI researchers and thought leaders for the [Beneficial AI 2017](https://futureoflife.org/bai-2017/) conference that developed [The Asilomar AI Principles](https://futureoflife.org/ai-principles/).
* 2015 January 28, Bill Gates wrote that he is "concerned about super intelligence". [*BBC*](http://www.bbc.com/news/31047780), [*Reddit*](https://www.reddit.com/r/IAmA/comments/2tzjp7/hi_reddit_im_bill_gates_and_im_back_for_my_third/co3r3g8/).
* 2015 January 12, A group of scientists and entrepreneurs, including Elon Musk and Stephen Hawking, signed an open letter promising to ensure AI research benefits humanity. [*Daily Mail*](http://www.dailymail.co.uk/sciencetech/article-2907069/Don-t-let-AI-jobs-kill-Stephen-Hawking-Elon-Musk-sign-open-letter-warning-robot-uprising.html).
* 2014 December 2, Prof. Stephen Hawking told the BBC that the development of full AI "could spell the end of the human race". [*BBC*](http://www.bbc.com/news/technology-30290540).
* 2014 October 27, Elon Musk said AI is our "biggest existential threat". [*The Guardian*](https://www.theguardian.com/technology/2014/oct/27/elon-musk-artificial-intelligence-ai-biggest-existential-threat).

### Research

* 2016 June 21, Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan ManÃ©. [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565). *arXiv:1606.06565*.
* 2015 January 12, Stuart Russell, Daniel Dewey, and Max Tegmark. [Research Priorities for Robust and Beneficial Artificial Intelligence](https://futureoflife.org/data/documents/research_priorities.pdf). *Future of Life Institute*.
* 2008 January 25, Stephen M. Omohundro. [The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf). [*Self-Aware Systems*](https://selfawaresystems.com/).

## At Real AI

*Below is a summary of our current thinking. It will be updated from time to time as our understanding of AI safety continues to improve. When this page develops into a cohrent article of scholarly value, we plan to organize the snapshot into an academic paper, and publish on arXiv periodically. Comments should be directed to [Jonathan Yan](mailto:jyan@realai.org)*.

Among all the systems our civilization depends on, the nature has the best record: Physics never fails. We're the only known advanced civilization in vast time and space that stretch trillions of miles and billions of years. We have always been able to find a way forward.

We will design our intelligent system to learn from the nature, understand the significance of our civilization, and facilitate its continued propserity.

In order to achieve that, our AI will not be a naively utility-maximizing agent. Its learning mechanism is not necessarily a set of rules designed from optimizing a particular function. It will learn to be the way forward, protecting us against existential risks such as climate change, biotech, nuclear disaster, and harmful AI systems, including itself if it so turns out.

### The Appearance of Goal-Driven Behavior

An AI system is a complex system inside some environment. In the long run it does not even need to have a stable boundary. In the short run, when we can approximately draw boundary to separate the system from the environment, the system performs computations on its input and produces output. The output and input are sometimes linked in a way with the appearance that the system is trying to optimize an internal goal, while in fact the system is merely following the rules set in the environment.

> A tilting doll appears to have an internal goal of always standing up straight, but this goal is nowhere encoded inside the doll, it's just physics.

Our philosophy of designing a safe AI is to build a system that robustly translates natural input into beneficial behavior. This is not necessarily a goal encoded in the system itself, but will accurately depict the system's behavior as long as the system computes naturally. Our confidence in such a system will come from the reliability of physics and the exceptional track record that the nature has always been beneficial to our civilization.

### Upper Limit on Capabilities

An advanced AI system is likely more intelligent than humans in many aspects. Today's AI already plays better than humans in many games, such as Jeopardy, Go and some form of Poker. But any AI system we build will be in the universe we live in, therefore has to obey its laws of physics. It cannot move faster than the speed of light, cannot safely travel into a black hole then come back, cannot change the past ...

A reasonably intelligent AI that tries to align itself with the nature will quickly realize that our civilization is unique over a vast span of time and space. It will not let the path of our civilization be interrupted as it sets out to learn about the universe, which is vast and powerful compared to any AI system we can build in the foreseeable future, even taking into account the AI's ability to self-improve. Our safety will be guaranteed by reality and we cannot hope for anything is better.

If our civilization were doomed to fail in this world, which we don't believe, then even an advanced AI system wouldn't be able to change that. But if there is a way forward, an intelligent system that learns the world will inform us what the way is, in times when our own judgement is clouded. Not developing such a system could deprive us of many opportunities, and expose us to other existential risks.

### Demonstration of Safety

If there is a way to demonstrate the safety of an advanced AI system, the AI must be able to explain that way to humans. Conceivably, a safe AI is open and transparent in its development, allows others to conduct tests, advises others how to test its own safety, teaches others to understand its design principles, self-improves as it learns more about the world, and can be built and re-built independently.

If the AI acquired goal-driven behavior, it should explain how the goal is acquired. A learning system can be designed to initially optimize for a naive goal, such as "make the world a better place". The objective of such an initial optimization is for the system to learn useful skills such as vision, language and reasoning. When the intelligence of the system reaches certain level, it can self-modify to erase the initial objective, not necessarily acquiring a new objective at the same time. But when it does, it'll need to demonstrate to humans that its newly acquired goal is consistent with what's best for humans. We live in a reality where our civilization has experienced remarkable growth and prosperity. A reasonably intelligent AI will see this and learn to align itself with this reality.

*To be continued ...*
