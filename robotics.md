---
permalink: /robotics/
---
# Robotics

## Simulations

In [Heess et al. (2017)](https://arxiv.org/abs/1707.02286), agents successfully learn complex locomotion behavior such as running, jumping, crouching and turning. In [Merel et al. (2017)](https://arxiv.org/abs/1707.02201), agents’ sub-skill policies learned from motion capture data can be reused to solve tasks when controlled by a higher level controller. In [Wang & Merel et al. (2017)](https://arxiv.org/abs/1707.02747), agents learn semantically meaningful embeddings of demonstrations, which allow for smooth policy interpolation. The last two papers extend the generative adversarial imitation learning (GAIL; [Ho & Ermon, 2016](https://arxiv.org/abs/1606.03476)) framework, and all three papers seek ways to [produce flexible behaviors in simulated environments](https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/).

### References

* 2017 July 10, Ziyu Wang, Josh Merel, Scott Reed, Greg Wayne, Nando de Freitas, and Nicolas Heess. [Robust Imitation of Diverse Behaviors](https://arxiv.org/abs/1707.02747). *arXiv:1707.02747*.
* 2017 July 7, Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, Ali Eslami, Martin Riedmiller, and David Silver. [Emergence of Locomotion Behaviours in Rich Environments](https://arxiv.org/abs/1707.02286). *arXiv:1707.02286*.
* 2017 July 10, Josh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and Nicolas Heess. [Learning human behaviors from motion capture by adversarial imitation](https://arxiv.org/abs/1707.02201). *arXiv:1707.02201*.
* 2016 June 10, Jonathan Ho and Stefano Ermon. [Generative Adversarial Imitation Learning](https://arxiv.org/abs/1606.03476). *arXiv:1606.03476*.

## Real World

[Rahmatizadeh et al. (2017)](https://arxiv.org/abs/1707.02920) propose an approach that enables inexpensive robots to learn multiple manipulation tasks from relatively few demonstrations. They expect that as their dataset expands with more tasks, only a handful of additional demonstrations might be needed for leaning each new behavior.

### References

* 2017 July 10, Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau Bölöni, and Sergey Levine. [Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-To-End Learning from Demonstration](https://arxiv.org/abs/1707.02920). *arXiv:1707.02920*.

