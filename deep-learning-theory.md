---
permalink: /deep-learning-theory/
---
# Deep Learning Theory

## Information Theory

In the plane of the mutual information (MI) between input and representation and the MI between representation and output (called the Information Plane), [Schwartz-Ziv & Tishby (2017)](https://arxiv.org/abs/1703.00810) visualized SGD in two stages, error minimization and representation compression. The compression stage covers most training epochs and begins after training errors get small.

## Physics

* 2017 April 5, Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. [Deep Learning and Quantum Physics : A Fundamental Bridge](https://arxiv.org/abs/1704.01552). *arXiv:1704.01552*.
* 2016 August 29, Henry W. Lin and Max Tegmark. [Why does deep and cheap learning work so well?](https://arxiv.org/abs/1608.08225). *arXiv:1608.08225*.
* 2014 November 30, Anna Choromanska, Mikael Henaff, Michael Mathieu, GÃ©rard Ben Arous, and Yann LeCun. [The Loss Surfaces of Multilayer Networks](https://arxiv.org/abs/1412.0233). *arXiv:1412.0233*.

## Topology

* 2014 January 6, Monica Bianchini and Franco Scarselli. [On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures](http://ieeexplore.ieee.org/document/6697897/). *IEEE Transactions on Neural Networks and Learning Systems*, 25(8):1553-1565.

## References

* 2017 April 29, Ravid Shwartz-Ziv and Naftali Tishby. [Opening the Black Box of Deep Neural Networks via Information](https://arxiv.org/abs/1703.00810). *arXiv:1703.00810*.
