---
permalink: /deep-learning-theory/
---
# Deep Learning Theory

[Lin et al. (2017)](https://arxiv.org/abs/1608.08225) explore how physical properties frequently translate to simple neural networks, then argue that a deep neural network could be more efficient than a shallow one when the data is generated by a hierarchical statistical process. They prove it is exponentially more efficient to approximate the product of input variables using a deep network than a shallow one. This theorem is extended in [Rolnick & Tegmark (2017)](https://arxiv.org/abs/1705.05502) to include broad natural classes of multivariate polynomials.

[Shalev-Shwartz et al. (2017)](https://arxiv.org/abs/1703.07950) described four types of simple problems that are difficult for common deep learning methods.

## Information Theory

In the plane of the mutual information (MI) between input and representation and the MI between representation and output (called the Information Plane), [Schwartz-Ziv & Tishby (2017)](https://arxiv.org/abs/1703.00810) visualized SGD in two stages, error minimization and representation compression. The compression stage covers most training epochs and begins after training errors get small.

## Loss Surface Geometry

* 2017 April 21, Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. [Entropy-SGD: Biasing Gradient Descent Into Wide Valleys](https://arxiv.org/abs/1611.01838). *arXiv:1611.01838*.
* 2017 March 15, Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. [Sharp Minima Can Generalize For Deep Nets](https://arxiv.org/abs/1703.04933). *arXiv:1703.04933*.
* 2017 February 9, Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/abs/1609.04836). *arXiv:1609.04836*.
* 2014 November 30, Anna Choromanska, Mikael Henaff, Michael Mathieu, GÃ©rard Ben Arous, and Yann LeCun. [The Loss Surfaces of Multilayer Networks](https://arxiv.org/abs/1412.0233). *arXiv:1412.0233*.

## Physics

* 2017 April 5, Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. [Deep Learning and Quantum Physics : A Fundamental Bridge](https://arxiv.org/abs/1704.01552). *arXiv:1704.01552*.

## Topology

* 2014 January 6, Monica Bianchini and Franco Scarselli. [On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures](http://ieeexplore.ieee.org/document/6697897/). *IEEE Transactions on Neural Networks and Learning Systems*, 25(8):1553-1565.

## References

* 2017 May 16, David Rolnick and Max Tegmark. [The power of deeper networks for expressing natural functions](https://arxiv.org/abs/1705.05502). *arXiv:1705.05502*.
* 2017 May 2, Henry W. Lin, Max Tegmark, and David Rolnick. [Why does deep and cheap learning work so well?](https://arxiv.org/abs/1608.08225). *arXiv:1608.08225*.
* 2017 April 29, Ravid Shwartz-Ziv and Naftali Tishby. [Opening the Black Box of Deep Neural Networks via Information](https://arxiv.org/abs/1703.00810). *arXiv:1703.00810*.
* 2017 April 26, Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. [Failures of Gradient-Based Deep Learning](https://arxiv.org/abs/1703.07950). *arXiv:1703.07950*.
