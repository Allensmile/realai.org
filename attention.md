---
permalink: /attention/
---
# Attention

## Visual Attention

[Li et al. (2017)](https://arxiv.org/abs/1703.10332) added an additional continue/stop action to the Recurrent Visual Attention Model ([Mnih et al., 2014](https://arxiv.org/abs/1406.6247)), and saved the average computation time.

[Ablavatski et al. (2017)](https://arxiv.org/abs/1706.03581) proposed a fully differentiable model that employs the Spatial Transformer ([Jaderberg et al., 2016](https://arxiv.org/abs/1506.02025)) as the visual attention mechanism.

## References

* 2017 June 12, Artsiom Ablavatski, Shijian Lu, and Jianfei Cai. [Enriched Deep Recurrent Visual Attention Model for Multiple Object Recognition](https://arxiv.org/abs/1706.03581). *arXiv:1706.03581*.
* 2017 May 16, Dieterich Lawson, George Tucker, Chung-Cheng Chiu, Colin Raffel, Kevin Swersky, and Navdeep Jaitly. [Learning Hard Alignments with Variational Inference](https://arxiv.org/abs/1705.05524). *arXiv:1705.05524*.
* 2017 March 30, Zhichao Li, Yi Yang, Xiao Liu, Shilei Wen, and Wei Xu. [Dynamic Computational Time for Visual Attention](https://arxiv.org/abs/1703.10332). *arXiv:1703.10332*. [code](https://github.com/baidu-research/DT-RAM).
* 2016 May 19, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473). *arXiv:1409.0473*.
* 2016 April 19, Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044). *arXiv:1502.03044*.
* 2016 February 4, Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025). *arXiv:1506.02025*.
* 2015 August 20, William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals. [Listen, Attend and Spell](https://arxiv.org/abs/1508.01211). *arXiv:1508.01211*.
* 2015 June 24, Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. [Attention-Based Models for Speech Recognition](https://arxiv.org/abs/1506.07503). *arXiv:1506.07503*.
* 2015 February 21, Yichuan Tang, Nitish Srivastava, and Ruslan Salakhutdinov. [Learning Generative Models with Visual Attention](https://arxiv.org/abs/1312.6110). *arXiv:1312.6110*.
* 2014 June 24, Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. [Recurrent Models of Visual Attention](https://arxiv.org/abs/1406.6247). *arXiv:1406.6247*.
